{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importing classes from display and pretty print modules\n",
    "from pprint import pprint\n",
    "from IPython.display import HTML\n",
    "from IPython.display import display\n",
    "#importing other necessary modules and packages\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from pymongo import MongoClient\n",
    "from operator import itemgetter\n",
    "import difflib\n",
    "from fuzzywuzzy import fuzz\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#For convenience imports are also included in individual cells where relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "from pprint import pprint\n",
    "\n",
    "#Setting up MongoDB connection\n",
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient(\"mongodb://localhost:27017\")\n",
    "db = client.osm\n",
    "#Creating db.bergen as a variable for the sake of brevity\n",
    "bergen = db.bergen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this document is to give a summary of the wrangling and analysis process performed in the OSM project, and to highlight interesting findings from that process. For more details about the data wrangling and data analysis processes, see <a href=\"http://htmlpreview.github.io/?https://github.com/gisledb/udacity_nanodegree/blob/master/ipython_notebooks/p3/osm_analysis.html\">osm_analysis.html</a> and <a href=\"http://htmlpreview.github.io/?https://github.com/gisledb/udacity_nanodegree/blob/master/ipython_notebooks/p3/osm_data_wrangling.html\">osm_data_wrangling.html</a>.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting on the project, my overall goal was to analyze the OpenStreetMaps (OSM) data for my hometown, Bergen, and hopefully discover some interesting findings during this process. As you will see later in the report, the main discoveries are related to data errors and structure of the user community."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-import Data Wrangling (audit and cleaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I started by going to OpenStreetMaps.com and finding the correct entity for the city of Bergen area I wanted to analyze. I settled on using the boundary type entity with boundary variable set to \"administrative\". Next I went to https://mapzen.com/data/metro-extracts/ to generate and download the necessary data file for Bergen.  \n",
    "\n",
    "Once I had the bergen.osm data file it was time for the pre-import wrangling process (full details <a href=\"http://htmlpreview.github.io/?https://github.com/gisledb/udacity_nanodegree/blob/master/ipython_notebooks/p3/osm_data_wrangling.html\">here</a>). I started by doing some experiments on a generated sample file, and settled on 2 focus areas for ensuring data quality in the pre-import cleaning phase:  \n",
    "1) ensuring good quality of postcodes, and correcting erroneous ones.  \n",
    "2) ensuring good quality of street names within Bergen.\n",
    "\n",
    "During the analysis, I discovered a few addresses with incorrect postcode format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 21641553\n",
      "name: Kiwi minipris\n",
      "shop: supermarket\n",
      "amenity: post_office\n",
      "nat_name: Kiwi minipris Frekhaug\n",
      "addr:city: Frekhaug\n",
      "wheelchair: yes\n",
      "addr:street: Havneveien\n",
      "addr:country: NO\n",
      "addr:postcode: NO-5918\n",
      "addr:housenumber: 36\n",
      "----\n",
      "id: 2698046129\n",
      "name: Data Respons AS (Bergen office)\n",
      "phone: +47 55 38 30 40\n",
      "source: http://datarespons.com/Company-test/Offices-and-people/Norway/Bergen/\n",
      "website: http://datarespons.com\n",
      "addr:city: Bergen\n",
      "addr:street: Edvard Griegs vei\n",
      "addr:postcode: NO-5059\n",
      "addr:housenumber: 3A\n",
      "----\n",
      "id: 2698046139\n",
      "name: Itslearning HQ\n",
      "phone: +47 55 23 60 70\n",
      "office: yes\n",
      "source: http://www.itslearning.eu/itslearning-hq-bergen-norway\n",
      "website: http://www.itslearning.eu\n",
      "addr:city: Bergen\n",
      "addr:street: Edvard Griegs vei\n",
      "addr:postcode: NO-5059\n",
      "addr:housenumber: 3A\n",
      "----\n",
      "id: 3645588506\n",
      "name: Circle K\n",
      "brand: Circle K\n",
      "amenity: fuel\n",
      "operator: Circle K Norge AS\n",
      "addr:city: Bergen\n",
      "addr:street: Helleveien\n",
      "addr:postcode: NO-5035\n",
      "addr:housenumber: 34\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "for _, element in ET.iterparse('data/bergen.osm'):\n",
    "    if element.tag == 'node':\n",
    "        tags = element.findall('tag')\n",
    "        for el in tags:\n",
    "            attrib_dict = el.attrib\n",
    "            if attrib_dict['k'] == 'addr:postcode':\n",
    "                if attrib_dict['v'][0:2] == 'NO':\n",
    "                    print(\"id:\",element.attrib['id'])\n",
    "                    for addr in tags:\n",
    "                        print(\"{0}: {1}\".format( addr.attrib['k'],addr.attrib['v'] ) )\n",
    "                    print('----')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that these four addresses have incorrect postcode format: They include the country abbreviation \"NO\", while we are only interested in the four digit postcode itself. I cleaned these erroneous records before importing the osm json file to MongoDB. During my analysis, I ensured that the postcodes were in fact corrected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21641553 {'city': 'Frekhaug', 'street': 'Havneveien', 'postcode': '5918', 'housenumber': '36', 'country': 'NO'}\n",
      "----\n",
      "2698046129 {'city': 'Bergen', 'street': 'Edvard Griegs vei', 'postcode': '5059', 'housenumber': '3A'}\n",
      "----\n",
      "2698046139 {'city': 'Bergen', 'street': 'Edvard Griegs vei', 'postcode': '5059', 'housenumber': '3A'}\n",
      "----\n",
      "3645588506 {'city': 'Bergen', 'street': 'Helleveien', 'postcode': '5035', 'housenumber': '34'}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "#After corrections, result from mongodb\n",
    "\n",
    "def print_address(_id):\n",
    "    if type(_id) == (str or int):\n",
    "        _id = [_id]\n",
    "    for item in _id:\n",
    "        item = str(item)\n",
    "        search = bergen.find_one( {'id': item} )\n",
    "        print(search['id'], search['address'] )\n",
    "        print('----')\n",
    "\n",
    "print_address([21641553, 2698046129,2698046139,3645588506])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/wikipedia_bergen_streets.jpg\" align=\"right\" width=\"290\">\n",
    "\n",
    "Next I had to come up with a good way for measuring street name quality. Since words and names in Norwegian often are concatenated I could not reuse the regex based strategy from the lecture videos. I decided to instead put the most common street name endings into a list which I compared all the street names in the data file too. I quickly discovered that there are way too many street names in Bergen which do not follow a common naming structure, so I realized I needed to come up with an additional strategy for the quality audit.\n",
    "\n",
    "I decided on a strategy of comparing the street names in the osm data file with street names from more or less official sources. I first scraped a Norwegian wikipedia article which listed all the street names in Bergen, with original data source being the Norwegian Mapping Authority. Since the list was quite dated (from 2005), I decided to look for alternative sources as well. I discovered that the Norwegian Public Roads Administration (NPRA) has a public API containing all the official Norwegian streetnames, which I used to generate a second list of Bergen street names.\n",
    "\n",
    "I then combined the street names from the two sources and removed any duplicates, ending up with a list of 2093 Bergen street names. At that point I felt I had a good foundation to continue with the quality audit.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I first compared the osm data set to my list of Bergen street names, the search returned an unexpected high number of non-matched osm street names. While spot checking the street names, I noticed that several of them were located outside of the city of Bergen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '21641553', 'lon': '5.2397614', 'lat': '60.5176144', 'uid': '1694', 'version': '6', 'changeset': '12440624', 'timestamp': '2012-07-22T21:23:45Z', 'user': 'M E Menk'}\n",
      "{'k': 'addr:city', 'v': 'Frekhaug'}\n",
      "{'k': 'addr:street', 'v': 'Havneveien'}\n",
      "{'k': 'addr:country', 'v': 'NO'}\n",
      "{'k': 'addr:postcode', 'v': 'NO-5918'}\n",
      "{'k': 'addr:housenumber', 'v': '36'}\n"
     ]
    }
   ],
   "source": [
    "for _, element in ET.iterparse('data/bergen.osm'):\n",
    "    if element.tag == 'node':\n",
    "        if element.attrib['id'] == '21641553':\n",
    "            print(element.attrib)\n",
    "            for el in element:\n",
    "                if el.attrib['k'][0:4] == 'addr':\n",
    "                    print(el.attrib)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To dig deeper into this, I downloaded an offical dataset from the Norwegian postal service containing all the postcodes in Norway, postcode name and municipality. When I compared the postal service postcodes to all the postcodes in the osm dataset, I discovered that some of the osm documents are located in cities outside of the municipality of Bergen.\n",
    "\n",
    "Since I mainly focus on Bergen in this project, I decided to limit my address corrections to adresses within Bergen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top rows of official postcode records:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>postal_code</th>\n",
       "      <th>postal_place</th>\n",
       "      <th>muni_number</th>\n",
       "      <th>muni_name</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0010</td>\n",
       "      <td>OSLO</td>\n",
       "      <td>301</td>\n",
       "      <td>OSLO</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0015</td>\n",
       "      <td>OSLO</td>\n",
       "      <td>301</td>\n",
       "      <td>OSLO</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  postal_code postal_place  muni_number muni_name category\n",
       "0        0010         OSLO          301      OSLO        B\n",
       "1        0015         OSLO          301      OSLO        B"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addresses in source file located in Bergen: 70286\n",
      "Addresses in source file located outside Bergen: 14099\n",
      "Incorrect postcodes in source file: ['NO-5918', 'NO-5059', 'NO-5059', 'NO-5035', 'NO-5059']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "postcodes_per_municipality = pd.read_csv('data/Postnummerregister_ansi.tsv', encoding='utf-8',delimiter='\\t',header=0, names=[\n",
    "        'postal_code','postal_place','muni_number','muni_name','category'],\n",
    "            dtype = {'postal_code': str, 'municipality_number': str})\n",
    "\n",
    "print(\"Top rows of official postcode records:\")\n",
    "display(postcodes_per_municipality.head(2))\n",
    "\n",
    "def is_postcode(elem):\n",
    "    return (elem.attrib.setdefault('k',None) == \"addr:postcode\")\n",
    "\n",
    "postcodes_bergen = set(postcodes_per_municipality[\n",
    "    postcodes_per_municipality['muni_name'] == 'BERGEN']['postal_code'] )\n",
    "postcodes_outside_bergen = set(postcodes_per_municipality[postcodes_per_municipality[\n",
    "    'muni_name'] != 'BERGEN']['postal_code'] )\n",
    "\n",
    "\n",
    "in_bergen_count = 0\n",
    "outside_bergen_count = 0\n",
    "postcodes_not_found = list()\n",
    "outside_dict = defaultdict(int)\n",
    "\n",
    "\n",
    "for _, element in ET.iterparse('data/bergen.osm'):\n",
    "    if element.tag in ['way','node']:\n",
    "        for tag in element.iter(\"tag\"):\n",
    "            if is_postcode(tag):\n",
    "                postcode = tag.attrib['v']\n",
    "                if postcode in postcodes_bergen:\n",
    "                    in_bergen_count += 1\n",
    "                elif postcode in postcodes_outside_bergen:\n",
    "                    outside_bergen_count += 1\n",
    "                    outside_dict[postcode] += 1\n",
    "                else:\n",
    "                    postcodes_not_found.append(postcode)\n",
    "                    \n",
    "print(\"Addresses in source file located in Bergen:\",in_bergen_count)\n",
    "print(\"Addresses in source file located outside Bergen:\",outside_bergen_count)\n",
    "print(\"Incorrect postcodes in source file:\",postcodes_not_found)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used the postal service postcode dataset to improve my postcode audit, and except for the already mentioned postcode errors, all the postcodes in the osm dataset matched postcodes in the official postcode dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After deciding on the criteria for the street name audit, I compared the osm street names to the street names from the official sources, and the common street name suffixes. This resulted in 15 osm street names within Bergen not found in the official street name dataset. I manually reviewed these 15 street names, and excluded 6 of these from further corrections. One of these are actually a correct street name, verified through online research, while 5 of these cannot easily be corrected (post box address, name of shopping center, unknown street names)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 9 remaining street names:\n",
      "['Hesthaugvn.',\n",
      " 'Christies Gate',\n",
      " 'Steinsvikvegen 430',\n",
      " 'Smøråshøgda 9',\n",
      " 'Minde alle',\n",
      " 'Thormøhlens Gate',\n",
      " 'Vilhelm Bjerknesvei 4-10',\n",
      " 'Tokanten',\n",
      " 'Laguneveien 1']\n",
      "Corrected using the following critera:\n",
      "{' Gate': ' gate',\n",
      " ' alle': ' allé',\n",
      " 'Tokanten': 'Nesttunveien',\n",
      " 'vei 4-10': 'vei',\n",
      " 'vn.': 'vegen'}\n"
     ]
    }
   ],
   "source": [
    "mapping = mapping = { \" Gate\": \" gate\", \" alle\": \" allé\", \"vn.\": \"vegen\",\n",
    "           \"Tokanten\": \"Nesttunveien\", \"vei 4-10\": \"vei\"\n",
    "            }\n",
    "error_streets = ['Hesthaugvn.',\n",
    " 'Christies Gate',\n",
    " 'Steinsvikvegen 430',\n",
    " 'Smøråshøgda 9',\n",
    " 'Minde alle',\n",
    " 'Thormøhlens Gate',\n",
    " 'Vilhelm Bjerknesvei 4-10',\n",
    " 'Tokanten',\n",
    " 'Laguneveien 1']\n",
    "\n",
    "print(\"The 9 remaining street names:\")\n",
    "pprint(error_streets)\n",
    "\n",
    "print(\"Corrected using the following critera:\")\n",
    "pprint(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the criteria for the address audit were finalized and implemented, I created and ran a function to generate a json file to import to mongodb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bergen street name errors imported to MongoDB: 0\n"
     ]
    }
   ],
   "source": [
    "#Checking if errors have been imported to MongoDB (expecting 0 results)\n",
    "\n",
    "errors_in_mongodb = bergen.find( { 'address.street': { \n",
    "    '$in': error_streets\n",
    "} \n",
    "                 } )\n",
    "\n",
    "print(\"Bergen street name errors imported to MongoDB:\",errors_in_mongodb.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following section is a summary of the analysis in osm_analysis.ipynb/osm_analysis.html. My main focus is quality of addresses in Bergen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Statistics\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "**File Sizes**  \n",
    "\n",
    "bergen.osm ........ 142.99 MB (original file)  \n",
    "bergen.osm.json ... 157.24 MB (file imported to mongodb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Document Types**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents in database: 681172\n",
      "Nodes in database: 628779\n",
      "Ways in database: 52393\n"
     ]
    }
   ],
   "source": [
    "print(\"Documents in database:\",bergen.count() )\n",
    "print(\"Nodes in database:\",bergen.find({ 'type': 'node' }).count() )\n",
    "print(\"Ways in database:\",bergen.find({ 'type': 'way' }).count() ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Addresses**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents in database with address information: 84625\n",
      "Documents in database with addresses within Bergen municipality: 70290\n"
     ]
    }
   ],
   "source": [
    "address_count = bergen.find ( { 'address': {'$exists': True} } ).count()\n",
    "bergen_count = bergen.find ( { 'address.postcode': {'$in': list(postcodes_bergen) } } ).count()\n",
    "\n",
    "print(\"Documents in database with address information:\", address_count)\n",
    "print(\"Documents in database with addresses within Bergen municipality:\",bergen_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Hunting####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting a general feel of the address data in the dataset, I went on to look for misspelled streetnames. My strategy was to look at streets with low address counts with similiar names to other streets in the database. To find near-matching street names I used a Python library named Fuzzywuzzy. After experimenting a little I settled on a fairly high fuzzy score of 90 and an address count of 10 as criteria for further analysis. \n",
    "\n",
    "In the end I ended up correcting the street names in 28 documents containing 15 different misspelled street names, and I exported 4 additional street names to research_street_spellings.csv which require more research than the scope of this project allows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Code from osm_analysis.html (and .ipynb) used to correct the misspelled street names in the database.*  \n",
    "  \n",
    "`\n",
    "for inx,name in df_misspelled_streets.iterrows():\n",
    "    bergen.update_many( { 'address.street': name[2]},\n",
    "                  { '$set': {'address.street': name[0] } \n",
    "                  } )\n",
    " `     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totlandsveien 0\n",
      "Totlandsvegen 212\n",
      "----\n",
      "Haakon Shetelings plass 0\n",
      "Haakon Sheteligs plass 8\n",
      "----\n",
      "Dreggsallmenning 0\n",
      "Dreggsallmenningen 11\n",
      "----\n",
      "Vestre murallmenningen 0\n",
      "Vestre Murallmenningen 28\n",
      "----\n",
      "Haakon Shetelings plass 0\n",
      "Haakon Sheteligs plass 8\n",
      "----\n",
      "Dreggsallmenning 0\n",
      "Dreggsallmenningen 11\n",
      "----\n",
      "Vilhelm Bjerknesvei 0\n",
      "Vilhelm Bjerknes’ vei 120\n",
      "----\n",
      "Travparkveien 0\n",
      "Travparkvegen 4\n",
      "----\n",
      "Travparkveien 0\n",
      "Travparkvegen 4\n",
      "----\n",
      "Børnesskogen 1\n",
      "Bønesskogen 222\n",
      "----\n",
      "C.Sundtsgate 0\n",
      "C. Sundts gate 55\n",
      "----\n",
      "Lars Hillesgate 0\n",
      "Lars Hilles gate 27\n",
      "----\n",
      "Østre Mulelvsmauet 3\n",
      "Vestre Mulelvsmauet 4\n",
      "----\n",
      "Herman Foss' gate 0\n",
      "Herman Foss’ gate 10\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "#Ensuring all is corrected. First result should return count 0\n",
    "misspelled_street_names = [{'correct_name': 'Totlandsvegen', 'wrong_name': 'Totlandsveien'},\n",
    " {'correct_name': 'Haakon Sheteligs plass',\n",
    "  'wrong_name': 'Haakon Shetelings plass'},\n",
    " {'correct_name': 'Dreggsallmenningen', 'wrong_name': 'Dreggsallmenning'},\n",
    " {'correct_name': 'Vestre Murallmenningen',\n",
    "  'wrong_name': 'Vestre murallmenningen'},\n",
    " {'correct_name': 'Haakon Sheteligs plass',\n",
    "  'wrong_name': 'Haakon Shetelings plass'},\n",
    " {'correct_name': 'Dreggsallmenningen', 'wrong_name': 'Dreggsallmenning'},\n",
    " {'correct_name': 'Vilhelm Bjerknes’ vei',\n",
    "  'wrong_name': 'Vilhelm Bjerknesvei'},\n",
    " {'correct_name': 'Travparkvegen', 'wrong_name': 'Travparkveien'},\n",
    " {'correct_name': 'Travparkvegen', 'wrong_name': 'Travparkveien'},\n",
    " {'correct_name': 'Bønesskogen', 'wrong_name': 'Børnesskogen'},\n",
    " {'correct_name': 'C. Sundts gate', 'wrong_name': 'C.Sundtsgate'},\n",
    " {'correct_name': 'Lars Hilles gate', 'wrong_name': 'Lars Hillesgate'},\n",
    " {'correct_name': 'Vestre Mulelvsmauet', 'wrong_name': 'Østre Mulelvsmauet'},\n",
    " {'correct_name': 'Herman Foss’ gate', 'wrong_name': \"Herman Foss' gate\"}]\n",
    "\n",
    "for name in misspelled_street_names:\n",
    "    print(name['wrong_name'],bergen.find( { 'address.street': name['wrong_name']}).count() )\n",
    "    print(name['correct_name'],bergen.find( { 'address.street': name['correct_name']}).count() )\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I looked for duplicate addresses in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count': 916, '_id': 'null'}\n"
     ]
    }
   ],
   "source": [
    "#Finding duplicate addresses\n",
    "\n",
    "pipeline = [\n",
    "    { '$group': { \n",
    "            '_id': { 'street': '$address.street', 'housenumber': '$address.housenumber' }, \n",
    "                'postcodes': { '$addToSet': '$address.postcode' }, \n",
    "            'count': {'$sum': 1 }\n",
    "            }\n",
    "        },\n",
    "    { '$match': {'count': {'$gt': 1} } },\n",
    "    { '$group': { '_id': 'null', 'count': { '$sum': 1 } } } ]\n",
    "\n",
    "for row in bergen.aggregate(pipeline):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found quite a few duplicate addresses in the Bergen osm dataset. In many cases the duplicate addresses seems to be there due to individual businesses located at the same address is often listed with its own address, instead of having a node referencing the address document.\n",
    "\n",
    "Due to vague OSM policies it is unclear whether to consider this as incorrect data. In the cases of multiple businesses at the same address having the same address, the OSM wiki states: \"However, there is still some debate on that point (see for example Address information in POI *and* building? on help.openstreetmap.org). Also, the community in some countries has established their own rules.\"\n",
    "\n",
    "To address the duplicate issue in detail, I suggest following up by looking at the individual duplicate addresses. One could for example start by looking at the three streets with the most individual duplicate addresses to see if there are any useful patterns to be found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Contributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I had a look at the distribution of user edits in the Bergen OSM data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total user count: 399\n",
      "Average contributions per user: 1707.2\n",
      "Median contributions per user: 11\n",
      "Mode of contribution count: 76 contributors (19.05%) submitted 1 edit.\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "user_count_query = bergen.aggregate( [\n",
    "   {\n",
    "     '$group': {\n",
    "        '_id' : { 'uid': '$created.uid', 'username': '$created.user' }\n",
    "           }\n",
    "        },\n",
    "   {\n",
    "     '$group': {\n",
    "        '_id': 'null',\n",
    "        'count': { '$sum': 1 }\n",
    "     }\n",
    "   }\n",
    "] )\n",
    "\n",
    "for doc in user_count_query:\n",
    "    user_count = doc['count']\n",
    "\n",
    "average_contributions = bergen.aggregate( [\n",
    "   {\n",
    "          '$group': \n",
    "            {\n",
    "                '_id' : \n",
    "                { 'uid': '$created.uid', 'username': '$created.user' },\n",
    "                'count': { '$sum': 1 } \n",
    "            } \n",
    "    },\n",
    "    { \n",
    "            '$group': \n",
    "            {\n",
    "                '_id': 'null',\n",
    "                'avg': { '$avg': '$count' } \n",
    "            }\n",
    "    }\n",
    "] )\n",
    "\n",
    "for doc in average_contributions:\n",
    "    user_average = round(doc['avg'],2)\n",
    "    \n",
    "grouped_users = list(bergen.aggregate([  \n",
    "        { \n",
    "            \"$group\" : \n",
    "            { \n",
    "                \"_id\" : { \"uid\": \"$created.uid\", \"username\": \"$created.user\" },\n",
    "                \"count\" : { \"$sum\" : 1} \n",
    "            } \n",
    "        },\n",
    "        { \"$sort\" : { \"count\" : 1 } }\n",
    "        ]))\n",
    "\n",
    "user_no = 0\n",
    "halfway = round(user_count / 2)\n",
    "mode_dict = defaultdict(int)\n",
    "\n",
    "for doc in grouped_users:\n",
    "        user_no += 1\n",
    "        val = doc['count']\n",
    "        if user_no == halfway:\n",
    "            user_median = val\n",
    "        \n",
    "        mode_dict[val] += 1\n",
    "\n",
    "\n",
    "        \n",
    "user_mode = max(mode_dict.items(), key=lambda a: a[1])\n",
    "mode_percentage = round((user_mode[1] / user_count) * 100,2)\n",
    "            \n",
    "print(\"Total user count:\",user_count)\n",
    "print(\"Average contributions per user:\",user_average)\n",
    "print(\"Median contributions per user:\",user_median)\n",
    "print(\"Mode of contribution count: {0} contributors ({1}%) submitted {2} edit.\".format(\n",
    "    user_mode[1],mode_percentage,user_mode[0] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the difference between the median and the average I suspected that the OSM community has a few heavy contributors working on the Bergen data. Further investigations found this to be true - the top 10 users in the dataset contributed more than 80% of the data edits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count': 140794, '_id': {'uid': '2114448', 'username': 'FredrikLindseth_import'}}\n",
      "{'count': 133655, '_id': {'uid': '2836853', 'username': 'frokor_import'}}\n",
      "{'count': 80243, '_id': {'uid': '103253', 'username': 'gormur'}}\n",
      "{'count': 39789, '_id': {'uid': '992708', 'username': 'Christian Madsen'}}\n",
      "{'count': 36440, '_id': {'uid': '722193', 'username': 'daviesp12'}}\n",
      "{'count': 31427, '_id': {'uid': '170061', 'username': 'frokor'}}\n",
      "{'count': 29969, '_id': {'uid': '1965308', 'username': 'FredrikLindseth'}}\n",
      "{'count': 22168, '_id': {'uid': '715936', 'username': 'Gazer75'}}\n",
      "{'count': 19287, '_id': {'uid': '3119148', 'username': 'cmeeren_import'}}\n",
      "{'count': 16081, '_id': {'uid': '8313', 'username': 'gisle'}}\n"
     ]
    }
   ],
   "source": [
    "top_users = list(bergen.aggregate([  \n",
    "        { \"$group\" : { \n",
    "                \"_id\" : { \"uid\": \"$created.uid\", \"username\": \"$created.user\" },\"count\" : { \"$sum\" : 1} } },\n",
    "        { \"$sort\" : { \"count\" : -1 } },\n",
    "         { \"$limit\" : 10 }\n",
    "    ]))\n",
    "\n",
    "for doc in top_users:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three of the top ten users have user names ending in \\_import, indicating their contributions were automated in some way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While finishing up my analysis, I got curious about the distribution of the various feature types for the documents in the mongodb database, so I decided to take a quick look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding all way features\n",
    "way_features = defaultdict(int)\n",
    "way_count = 0\n",
    "\n",
    "for doc in bergen.find( {'type': 'way'}):    \n",
    "\n",
    "    way_count += 1\n",
    "    for k,v in doc.items():\n",
    "        try:\n",
    "            for key,val in v.items():\n",
    "                way_features[k+'.'+key] += 1\n",
    "                \n",
    "        except AttributeError:\n",
    "            if type(v) == list:\n",
    "                for item in v:\n",
    "                    if type(item) == dict:\n",
    "                        for key,val in item.items():\n",
    "                            way_features[k+'.'+key] += 1\n",
    "                    else:\n",
    "                        way_features[k] += 1\n",
    "                \n",
    "            elif type(v) == str:\n",
    "                way_features[k] += 1\n",
    "\n",
    "#Finding all node features\n",
    "node_features = defaultdict(int)\n",
    "node_count = 0\n",
    "\n",
    "for doc in bergen.find({'type': 'node'}):    \n",
    "\n",
    "    node_count += 1\n",
    "    for k,v in doc.items():\n",
    "\n",
    "        try:\n",
    "            for key,val in v.items():\n",
    "                node_features[k+'.'+key] += 1\n",
    "        except AttributeError:\n",
    "            if type(v) == list:\n",
    "                for item in v:\n",
    "                    if type(item) == dict:\n",
    "                        for key,val in item.items():\n",
    "                            node_features[k+'.'+key] += 1\n",
    "                    else:\n",
    "                        node_features[k] += 1\n",
    "                \n",
    "            elif type(v) == str:\n",
    "                node_features[k] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of way documents: 52393\n",
      "number of node documents: 628779\n",
      "----\n",
      "Number of way feature types: 358\n",
      "Number of node feature types: 490\n",
      "Total unique feature types: 674\n"
     ]
    }
   ],
   "source": [
    "print(\"number of way documents:\",way_count)\n",
    "print(\"number of node documents:\",node_count)\n",
    "print(\"----\")\n",
    "print(\"Number of way feature types:\",len(way_features))\n",
    "print(\"Number of node feature types:\",len(node_features))\n",
    "print(\"Total unique feature types:\",len({**way_features,**node_features}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brief analysis of the feature types uncovered that several of the features types are used for the same purpose. I would therefore argue that community efforts should be made to clarify the correct feature type for a certain attribute, and to correct inconsistencies in the data set. An example of this is email and website information, which I will illustrate below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>way website features:</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url 1\n",
      "website 131\n",
      "no-kartverket-ssr:url 1\n",
      "contact.website 2\n",
      "website.en 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b>node website features:</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contact.website 9\n",
      "website 739\n",
      "source.url 10\n",
      "website.en 4\n",
      "url 11\n",
      "heritage.website 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b>way email features:</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contact.email 16\n",
      "email 30\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b>node email features:</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contact.email 26\n",
      "email 288\n"
     ]
    }
   ],
   "source": [
    "display(HTML(\"<b>way website features:</b>\"))\n",
    "for key in way_features:\n",
    "    if 'url' in key or 'website' in key:\n",
    "        print(key,way_features[key])\n",
    "\n",
    "display(HTML(\"<b>node website features:</b>\"))\n",
    "for key in node_features:\n",
    "    if 'url' in key or 'website' in key:\n",
    "        print(key,node_features[key])\n",
    "print()        \n",
    "#Looking for potential duplicate email feature types\n",
    "display(HTML(\"<b>way email features:</b>\"))\n",
    "for key in way_features:\n",
    "    if 'email' in key:\n",
    "        print(key,way_features[key])\n",
    "display(HTML(\"<b>node email features:</b>\"))\n",
    "for key in node_features:\n",
    "    if 'email' in key:\n",
    "        print(key,node_features[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eateries and Types of Cuisine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While investigating the feature types, I got interested in the cuisine information for eateries (mainly fast food, restaurants and cafés)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Cuisine, all eateries</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count': 54, '_id': {'cuisine': 'coffee_shop'}}\n",
      "{'count': 19, '_id': {'cuisine': 'burger'}}\n",
      "{'count': 17, '_id': {'cuisine': 'pizza'}}\n",
      "{'count': 14, '_id': {'cuisine': 'sushi'}}\n",
      "{'count': 13, '_id': {'cuisine': 'kebab'}}\n",
      "{'count': 10, '_id': {'cuisine': 'chinese'}}\n",
      "{'count': 9, '_id': {'cuisine': 'italian'}}\n",
      "{'count': 9, '_id': {'cuisine': 'thai'}}\n",
      "{'count': 6, '_id': {'cuisine': 'mexican'}}\n",
      "{'count': 5, '_id': {'cuisine': 'asian'}}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b>Cuisine, restaurants only</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count': 14, '_id': {'cuisine': 'pizza'}}\n",
      "{'count': 12, '_id': {'cuisine': 'sushi'}}\n",
      "{'count': 10, '_id': {'cuisine': 'chinese'}}\n",
      "{'count': 9, '_id': {'cuisine': 'thai'}}\n",
      "{'count': 8, '_id': {'cuisine': 'italian'}}\n",
      "{'count': 6, '_id': {'cuisine': 'mexican'}}\n",
      "{'count': 4, '_id': {'cuisine': 'burger'}}\n",
      "{'count': 3, '_id': {'cuisine': 'tapas'}}\n",
      "{'count': 3, '_id': {'cuisine': 'persian'}}\n",
      "{'count': 3, '_id': {'cuisine': 'indian'}}\n"
     ]
    }
   ],
   "source": [
    "#Most common food served at eatiries in Bergen\n",
    "\n",
    "#Counts of different type of eateries\n",
    "pipeline = [ \n",
    "    {'$match': {'cuisine': { '$exists': 1 } } },\n",
    "    { '$group': { \n",
    "        '_id': {'cuisine': '$cuisine' }, \n",
    "        'count': {'$sum': 1 } } \n",
    "    },\n",
    "    { '$sort': {'count': -1 } },\n",
    "    { '$limit': 10 }\n",
    "]\n",
    "\n",
    "display(HTML(\"<b>Cuisine, all eateries</b>\"))\n",
    "for doc in bergen.aggregate(pipeline):\n",
    "    print(doc)\n",
    "\n",
    "\n",
    "pipeline = [ \n",
    "    {'$match': {'amenity': 'restaurant', 'cuisine': {'$exists': 1} } },\n",
    "    { '$group': { \n",
    "        '_id': {'cuisine': '$cuisine' }, \n",
    "        'count': {'$sum': 1 } } \n",
    "    },\n",
    "    { '$sort': {'count': -1 } },\n",
    "    { '$limit': 10 }\n",
    "]\n",
    "print()\n",
    "display(HTML(\"<b>Cuisine, restaurants only</b>\"))\n",
    "for doc in bergen.aggregate(pipeline):\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Bergen restaurants pizza, sushi and chinese food are the top cuisines. Among all eateries with cuisine information, if we exclude coffee_shop, the top 3 cuisines are burger, pizza and sushi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eateries with cuisine information\n",
      "{'count': 94, '_id': {'eatery_type': 'restaurant'}}\n",
      "{'count': 56, '_id': {'eatery_type': 'cafe'}}\n",
      "{'count': 37, '_id': {'eatery_type': 'fast_food'}}\n",
      "    \n",
      "Eateries missing cuisine information\n",
      "{'count': 95, '_id': {'eatery_type': 'restaurant'}}\n",
      "{'count': 31, '_id': {'eatery_type': 'cafe'}}\n",
      "{'count': 11, '_id': {'eatery_type': 'fast_food'}}\n"
     ]
    }
   ],
   "source": [
    "#Counts of eatery types with cuisine information\n",
    "pipeline = [ \n",
    "    {'$match': {'cuisine':{ '$exists': 1 }, 'amenity': {'$in': ['cafe','restaurant', 'fast_food'] } } },\n",
    "    { '$group': { \n",
    "        '_id': {'eatery_type': '$amenity' }, \n",
    "        'count': {'$sum': 1 } } \n",
    "    },\n",
    "    {'$sort': {'count': -1 } }\n",
    "]\n",
    "\n",
    "print('Eateries with cuisine information')\n",
    "for doc in bergen.aggregate(pipeline):\n",
    "    print(doc)\n",
    "\n",
    "print('    ')\n",
    "#Counts of eatery types without cuisine information\n",
    "pipeline = [ \n",
    "    {'$match': {'cuisine':{ '$exists': 0 }, 'amenity': {'$in': ['cafe','restaurant', 'fast_food'] } } },\n",
    "    { '$group': { \n",
    "        '_id': {'eatery_type': '$amenity' }, \n",
    "        'count': {'$sum': 1 } } \n",
    "    },\n",
    "    {'$sort': {'count': -1 } }\n",
    "]\n",
    "\n",
    "print('Eateries missing cuisine information')\n",
    "for doc in bergen.aggregate(pipeline):\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite a few restaurants, cafés, and fast food places lack cuisine information. Improving this could be a minimal effort task with a high yield result for the active Bergen OSM contributors. However, keeping this information up to date and accurate might become a challenge in the future. Eateries frequently go out of business, and if they remain in the osm data for a long time after shutting down, cuisine information might make it more frustrating for users. For example, a user searching for pizza restaurants might not notice a closed pizza restaurant without cuisine information in the dataset, as it would not show up in the search results, but if it had cuisine listed as \"pizza\", there is a risk of users going to a location where a restaurant no longer exists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 184 address documents without street names in the dataset. Further investigation into those documents is recommended. \n",
    "\n",
    "To address the duplicate issue in detail, I suggest following up by looking at the individual duplicate addresses. One could, for example, start by looking at the three streets with the most individual duplicate addresses to see if there are any useful patterns to be found. Since a few users are very heavy contributors to the Bergen OSM data, it might also be worth searching for user patterns regarding the duplicate addresses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The community contributors have themselves marked 236 nodes which needs to be reviewed, by adding the field \"fixme\" with a description as the value. Prioritized efforts can be made to audit these, and correct them where needed."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
