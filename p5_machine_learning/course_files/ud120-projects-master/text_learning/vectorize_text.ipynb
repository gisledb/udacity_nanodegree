{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import sys\n",
    "\n",
    "sys.path.append( \"../tools/\" )\n",
    "from parse_out_email_text import parseOutText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Starter code to process the emails from Sara and Chris to extract\n",
    "    the features and get the documents ready for classification.\n",
    "\n",
    "    The list of all the emails from Sara are in the from_sara list\n",
    "    likewise for emails from Chris (from_chris)\n",
    "\n",
    "    The actual documents are in the Enron email dataset, which\n",
    "    you downloaded/unpacked in Part 0 of the first mini-project. If you have\n",
    "    not obtained the Enron email corpus, run startup.py in the tools folder.\n",
    "\n",
    "    The data is stored in lists and packed away in pickle files at the end.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from_sara  = open(\"from_sara.txt\", \"r\")\n",
    "from_chris = open(\"from_chris.txt\", \"r\")\n",
    "\n",
    "from_data = []\n",
    "word_data = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In vectorize_text.py, you will iterate through all the emails from Chris and from Sara. For each email, feed the opened email to parseOutText() and return the stemmed text string. Then do two things:\n",
    "\n",
    "- remove signature words (“sara”, “shackleton”, “chris”, “germani”--bonus points if you can figure out why it's \"germani\" and not \"germany\")\n",
    "- append the updated text string to word_data -- if the email is from Sara, append 0 (zero) to from_data, or append a 1 if Chris wrote the email.\n",
    "- Once this step is complete, you should have two lists: one contains the stemmed text of each email, and the second should contain the labels that encode (via a 0 or 1) who the author of that email is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "### temp_counter is a way to speed up the development--there are\n",
    "### thousands of emails from Sara and Chris, so running over all of them\n",
    "### can take a long time\n",
    "### temp_counter helps you only look at the first 200 emails in the list so you\n",
    "### can iterate your modifications quicker\n",
    "temp_counter = 0\n",
    "\n",
    "\n",
    "# Importing nltk stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = stopwords.words(\"english\")\n",
    "\n",
    "for name, from_person in [(\"sara\", from_sara), (\"chris\", from_chris)]:\n",
    "    for path in from_person:\n",
    "        ### only look at first 200 emails when developing\n",
    "        ### once everything is working, remove this line to run over full dataset\n",
    "#         temp_counter += 1\n",
    "#         if temp_counter < 200:\n",
    "        path = os.path.join('..', path[:-1])\n",
    "        #print(path)\n",
    "        email = open(path, \"r\")\n",
    "\n",
    "        ### use parseOutText to extract the text from the opened email        \n",
    "        stemmed_email = parseOutText(email)\n",
    "\n",
    "        # step 1: Remove signature words\n",
    "#             updated_text = str()\n",
    "#             for word in stemmed_email.split():\n",
    "#                 if word not in stopwords:\n",
    "#                     updated_text = updated_text + word + \" \"            \n",
    "\n",
    "        ### use str.replace() to remove any instances of the words\n",
    "        ### [\"sara\", \"shackleton\", \"chris\", \"germani\"]\n",
    "        signature_words = [\"sara\", \"shackleton\", \"chris\", \"germani\"]\n",
    "        for word in signature_words:\n",
    "            stemmed_email = stemmed_email.replace(word, '')\n",
    "\n",
    "        ### append the text to word_data\n",
    "        word_data.append(stemmed_email)\n",
    "        if name == \"sara\":\n",
    "            from_data.append(0)\n",
    "        elif name == \"chris\":\n",
    "            from_data.append(1)\n",
    "        else:\n",
    "            print(\"NAME ERROR: Unknown person!\")\n",
    "            print(name)\n",
    "            break\n",
    "        ### append a 0 to from_data if email is from Sara, and 1 if email is from Chris\n",
    "\n",
    "\n",
    "        email.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tjonesnsf i will take off the monday and tuesday prior to thanksgiv pleas advis if ani problem  '"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_data[152]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emails processed\n"
     ]
    }
   ],
   "source": [
    "print(\"emails processed\")\n",
    "from_sara.close()\n",
    "from_chris.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump( word_data, open(\"your_word_data.pkl\", \"wb\") )\n",
    "# pickle.dump( from_data, open(\"your_email_authors.pkl\", \"wb\") )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<17577x38757 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1078797 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### in Part 4, do TfIdf vectorization here\n",
    "\n",
    "#Importing vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "vectorizer.fit_transform(word_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38757"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word no. 34597: stephaniethank\n"
     ]
    }
   ],
   "source": [
    "print(\"word no. 34597:\",vectorizer.get_feature_names()[34597])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
